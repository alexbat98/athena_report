@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
month = {mar},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@inproceedings{Arvind1987,
abstract = {The MIT Tagged-Token Dataflow project has a radical, but integrated approach to high-performance parallel computing. We study high-level languages that have fine-grained parallelism implicitly in their operational semantics, rather than extending conventional sequential languages. Programs are compiled to dataflow graphs which may be regarded as a parallel machine language. Dataflow graphs are executed on the MIT Tagged-Token Dataflow Architecture, which is a multiprocessor architecture. In this paper we provide an overview of this approach.},
author = {Arvind and Nikhil, Rishiyur S.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/3-540-17945-3_1},
isbn = {9783540179450},
issn = {16113349},
pages = {1--29},
publisher = {Springer Verlag},
title = {{Executing a program on the MIT Tagged-Token Dataflow architecture}},
volume = {259 LNCS},
year = {1987}
}
@inproceedings{Dean2012,
abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
author = {Dean, Jeffrey and Corrado, Greg S and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V and Mao, Mark Z and Ranzato, Marc Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dean et al. - 2012 - Large Scale Distributed Deep Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
pages = {1223--1231},
title = {{Large scale distributed deep networks}},
volume = {2},
year = {2012}
}
@inproceedings{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
isbn = {9781450330633},
keywords = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
month = {nov},
pages = {675--678},
publisher = {Association for Computing Machinery, Inc},
title = {{Caffe: Convolutional architecture for fast feature embedding}},
url = {https://www.researchgate.net/publication/264979485},
year = {2014}
}
@inproceedings{Johnson2015,
author = {Johnson, Teresa and Li, David},
booktitle = {EuroLLVM 2015},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson, Li - Unknown - EuroLLVM 2015 ThinLTO A Fine-Grained Demand-Driven Infrastructure.pdf:pdf},
keywords = {llvm},
mendeley-tags = {llvm},
title = {{ThinLTO: A Fine-Grained Demand-Driven Infrastructure}},
url = {http://llvm.org/devmtg/2015-04/slides/ThinLTO{\_}EuroLLVM2015.pdf},
year = {2015}
}
@book{Lopes2014,
abstract = {LLVM is a bleeding edge compiler technology framework. Easily extendable and designed as a multitude of libraries, LLVM provides a smooth experience for compiler newcomers and reduces the steep learning curve often associated with compiler development. To start, this book will show you how to configure, build, and install LLVM libraries, tools, and external projects. Next, you will be introduced to LLVM design and how it works in practice throughout each LLVM compiler stage: frontend, IR, backend, the JIT engine, cross-compilation capabilities, and the plugin interface. With multiple hands-on examples and source code snippets, Getting Started with LLVM Core Libraries ensures a solid and smooth first step into the LLVM compiler development environment.},
author = {Lopes, Bruno Cardoso and Auler, Rafael},
booktitle = {Spring},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopes, Auler - 2014 - Getting Started with LLVM Core Libraries Get to grips with LLVM essentials and use the core libraries to build adv.pdf:pdf},
isbn = {9781783287031},
pages = {1--6},
title = {{Getting Started with LLVM Core Libraries Get to grips with LLVM essentials and use the core libraries to build advanced tools}},
url = {www.packtpub.com},
year = {2014}
}
@inproceedings{Ma2017,
abstract = {We develop a scalable and extendable training framework that can utilize GPUs across nodes in a cluster and accelerate the training of deep learning models based on data parallelism. Both synchronous and asynchronous training are implemented in our framework, where parameter exchange among GPUs is based on CUDA-aware MPI. In this report, we analyze the convergence and capability of the framework to reduce training time when scaling the synchronous training of AlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel ways to reduce the communication overhead caused by exchanging parameters. Finally, we release the framework as open-source for further research on distributed deep learning (https://github.com/ uoguelph-mlrg/Theano-MPI).},
archivePrefix = {arXiv},
arxivId = {1605.08325},
author = {Ma, He and Mao, Fei and Taylor, Graham W},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-58943-5_64},
eprint = {1605.08325},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma, Mao, Taylor - Unknown - Theano-MPI a Theano-based Distributed Training Framework.pdf:pdf},
isbn = {9783319589428},
issn = {16113349},
pages = {800--813},
title = {{Theano-MPI: A Theano-based distributed training framework}},
url = {https://github.com/uoguelph-mlrg/Theano-MPI},
volume = {10104 LNCS},
year = {2017}
}
@article{Rotem2018,
abstract = {This paper presents the design of Glow, a machine learning compiler for heterogeneous hardware. It is a pragmatic approach to compilation that enables the generation of highly optimized code for multiple targets. Glow lowers the traditional neural network dataflow graph into a two-phase strongly-typed intermediate representation. The high-level intermediate representation allows the optimizer to perform domain-specific optimizations. The lower-level instruction-based address-only intermediate representation allows the compiler to perform memory-related optimizations, such as instruction scheduling, static memory allocation and copy elimination. At the lowest level, the optimizer performs machine-specific code generation to take advantage of specialized hardware features. Glow features a lowering phase which enables the compiler to support a high number of input operators as well as a large number of hardware targets by eliminating the need to implement all operators on all targets. The lowering phase is designed to reduce the input space and allow new hardware backends to focus on a small number of linear algebra primitives.},
archivePrefix = {arXiv},
arxivId = {1805.00907},
author = {Rotem, Nadav and Fix, Jordan and Abdulrasool, Saleem and Catron, Garret and Deng, Summer and Dzhabarov, Roman and Gibson, Nick and Hegeman, James and Lele, Meghan and Levenstein, Roman and Montgomery, Jack and Maher, Bert and Nadathur, Satish and Olesen, Jakob and Park, Jongsoo and Rakhov, Artem and Smelyanskiy, Misha and Wang, Man},
eprint = {1805.00907},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rotem et al. - Unknown - Glow Graph Lowering Compiler Techniques for Neural Networks.pdf:pdf},
journal = {arxiv.org},
title = {{Glow: Graph Lowering Compiler Techniques for Neural Networks}},
url = {http://github.com/pytorch/glow http://arxiv.org/abs/1805.00907},
year = {2018}
}
@article{Tang2018,
abstract = {For a deep learning model, efficient execution of its computation graph is key to achieving high performance. Previous work has focused on improving the performance for individual nodes of the computation graph, while ignoring the parallelization of the graph as a whole. However, we observe that running multiple operations simultaneously without interference is critical to efficiently perform parallelizable small operations. The attempt of executing the computation graph in parallel in deep learning frameworks usually involves much resource contention among concurrent operations, leading to inferior performance on manycore CPUs. To address these issues, in this paper, we propose Graphi, a generic and high-performance execution engine to efficiently execute a computation graph in parallel on manycore CPUs. Specifically, Graphi minimizes the interference on both software/hardware resources, discovers the best parallel setting with a profiler, and further optimizes graph execution with the critical-path first scheduling. Our experiments show that the parallel execution consistently outperforms the sequential one. The training times on four different neural networks with Graphi are 2.1x to 9.5x faster than those with TensorFlow on a 68-core Intel Xeon Phi processor.},
archivePrefix = {arXiv},
arxivId = {1807.09667},
author = {Tang, Linpeng and Wang, Yida and Willke, Theodore L and Li, Kai},
eprint = {1807.09667},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tangˇ, et al. - Unknown - Scheduling Computation Graphs of Deep Learning Models on Manycore CPUs.pdf:pdf},
journal = {arxiv.org},
title = {{Scheduling Computation Graphs of Deep Learning Models on Manycore CPUs}},
url = {https://arxiv.org/abs/1807.09667 http://arxiv.org/abs/1807.09667},
year = {2018}
}
@article{theano,
author = {Team, The Theano Development and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'{e}}d{\'{e}}ric and Bayer, Justin and Belikov, Anatoly and Others},
journal = {arXiv preprint arXiv:1605.02688},
title = {{Theano: A Python framework for fast computation of mathematical expressions}},
year = {2016}
}
@article{Tokui2015,
abstract = {Software frameworks for neural networks play key roles in the development and application of deep learning methods. However, as new types of deep learning models are developed, existing frameworks designed for convolutional neural net-works are becoming less useful. In this paper, we introduce Chainer, a Python-based, standalone open source framework for deep learning models. Chainer pro-vides a flexible, intuitive, and high performance means of implementing a full range of deep learning models, including state-of-the-art models such as recurrent neural networks and variational autoencoders.},
author = {Tokui, Seiya and Oono, Kenta and Hido, Shohei and Clayton, Justin},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tokui et al. - Unknown - Chainer a Next-Generation Open Source Framework for Deep Learning.pdf:pdf},
journal = {learningsys.org},
title = {{Chainer: a Next-Generation Open Source Framework for Deep Learning}},
url = {http://learningsys.org/papers/LearningSys{\_}2015{\_}paper{\_}33.pdf},
year = {2015}
}
