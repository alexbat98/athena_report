@article{Gori1992,
abstract = {Supervised learning in multilayered neural networks (MLN's) has been recently proposed through the well-known backpropagation (BP) algorithm. This is a gradient method that can get stuck in local minima, as simple examples can show. In this paper, some conditions on the network architecture and the learning environment, which ensure the convergence of the BP algorithm, are proposed. It is proven in particular that the convergence holds if the classes are linearly separable. In this case, the experience gained in several experiments shows that MLN's exceed perceptrons in generalization to new examples. {\textcopyright} 1992 IEEE},
author = {Gori, Marco and Tesi, Alberto},
doi = {10.1109/34.107014},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Backpropagation,learning environment,linearly,multilayered networks,pattern recognition,separable classes},
number = {1},
pages = {76--86},
title = {{On the problem of local minima in backpropagation}},
volume = {14},
year = {1992}
}
@article{Attneave1950,
abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists--the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology--a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
author = {Attneave, Fred and B., M. and Hebb, D O},
doi = {10.2307/1418888},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Attneave, B., Hebb - 1950 - The Organization of Behavior A Neuropsychological Theory.pdf:pdf},
isbn = {9780805843002},
issn = {00029556},
journal = {The American Journal of Psychology},
number = {4},
pages = {633},
title = {{The Organization of Behavior; A Neuropsychological Theory}},
volume = {63},
year = {1950}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {08997667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
pmid = {9377276},
publisher = {MIT Press Journals},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, J. J.},
doi = {10.1073/pnas.79.8.2554},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hopfield - 1982 - Neural networks and physical systems with emergent collective computational abilities.pdf:pdf},
issn = {00278424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {apr},
number = {8},
pages = {2554--2558},
pmid = {6953413},
publisher = {National Academy of Sciences},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
volume = {79},
year = {1982}
}
@article{Hastad2014,
abstract = {We prove that the correlation of a depth-d unbounded fanin circuit of size S with parity of n variables is at most 2-$\Omega$(n/(log S)d-1).},
author = {H{\aa}stad, Johan},
doi = {10.1137/120897432},
issn = {00975397},
journal = {SIAM Journal on Computing},
keywords = {Circuits complexity,Parity,Small-depth circuits,Switching lemma},
number = {5},
pages = {1699--1708},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{On the correlation of parity and small-depth circuits}},
volume = {43},
year = {2014}
}
@article{LeCun,
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
journal={Proceedings of the IEEE},
volume={86}, 
number={11}, 
pages={2278-2324},
year = {1998},
title = {{Gradient-based learning applied to document recognition}},
url = {https://ieeexplore.ieee.org/abstract/document/726791/}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
month = {dec},
number = {4},
pages = {115--133},
publisher = {Kluwer Academic Publishers},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1958 American Psychological Association.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenblatt - 1958 - The perceptron A probabilistic model for information storage and organization in the brain.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
keywords = {AS INFORMATION STORAGE MODEL INFORMATION,IN BRAIN BRAIN,INFORMATION STORAGE IN,MODEL FOR,MODEL FOR LEARNING {\&} MEMORY,PERCEPTION,STORAGE},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
url = {https://psycnet.apa.org/journals/rev/65/6/386/},
volume = {65},
year = {1958}
}
@article{Yao1985,
abstract = {This paper studies the use of redundancy to enhance reliability for sorting and related networks built from unreliable comparators. Two models of fault-tolerant networks are discussed. The first model patterns after the concept of error-correcting codes in information theory, and the other follows the stochastic criterion used by J. von Neumann and E. F. Moore and C. Shannon. It is shown, for example, that an additional k(2n-3) comparators are sufficient to render a sorting network reliable, provided that no more than k of its comparators may be faulty.},
author = {Yao, Andrew C. and Yao, F. Frances},
doi = {10.1137/0214009},
issn = {00975397},
journal = {SIAM Journal on Computing},
number = {1},
pages = {120--128},
title = {{FAULT-TOLERANT NETWORKS FOR SORTING.}},
url = {https://epubs.siam.org/doi/abs/10.1137/0214009},
volume = {14},
year = {1985}
}
@book{minsky,
author = {Минский, М and Пейперт, С},
publisher = {Мир},
title = {{Персептроны: Пер. с англ}},
year = {1971}
}
@article{cyb,
author = {Cybenko, George},
journal = {Mathematics of control, signals and systems},
number = {4},
pages = {303--314},
publisher = {Springer},
title = {{Approximation by superpositions of a sigmoidal function}},
volume = {2},
year = {1989}
}
@book{deeplearning,
author = {Николенко, Сергей Игоревич and Кадурин, Артур Аликович and Архангельская, Екатерина Владиславовна},
isbn = {9785496025362},
publisher = {Издательский дом "Питер"},
title = {{Глубокое обучение}},
language={russian},
year = {2017}
}
@techreport{Delalleau,
abstract = {We investigate the representational power of sum-product networks (computation networks analogous to neural networks, but whose individual units compute either products or weighted sums), through a theoretical analysis that compares deep (multiple hidden layers) vs. shallow (one hidden layer) architectures. We prove there exist families of functions that can be represented much more efficiently with a deep network than with a shallow one, i.e. with substantially fewer hidden units. Such results were not available until now, and contribute to motivate recent research involving learning of deep sum-product networks, and more generally motivate research in Deep Learning.},
author = {Delalleau, Olivier and Bengio, Yoshua},
booktitle = {papers.nips.cc},
file = {:home/alex/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Delalleau, Bengio - Unknown - Shallow vs. Deep Sum-Product Networks.pdf:pdf},
title = {{Shallow vs. Deep Sum-Product Networks}},
url = {http://papers.nips.cc/paper/4350-shallow-vs-deep-sum-product-networks}
}
